# ============================================
# SIMPLE NETWORK ANOMALY DETECTOR
# ============================================
# JUST COPY-PASTE ALL OF THIS!
# ============================================

print("üöÄ Starting Simple Network Anomaly Detection System...")

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("üì¶ Libraries loaded successfully!")

# ======================
# STEP 1: LOAD DATA
# ======================

print("\nüìä STEP 1: Loading dataset...")

# Define column names for our dataset
columns = [
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes',
    'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',
    'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',
    'su_attempted', 'num_root', 'num_file_creations', 'num_shells',
    'num_access_files', 'num_outbound_cmds', 'is_host_login',
    'is_guest_login', 'count', 'srv_count', 'serror_rate',
    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',
    'dst_host_srv_count', 'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
    'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate', 'attack_type', 'difficulty_level'
]

try:
    # Load the dataset
    df = pd.read_csv('data/KDDTrain+.txt', names=columns)
    print(f"‚úÖ Dataset loaded! Shape: {df.shape}")
    print(f"   - Rows: {df.shape[0]}")
    print(f"   - Columns: {df.shape[1]}")
    
except FileNotFoundError:
    print("‚ùå Dataset not found! Running download script...")
    import download_data
    df = pd.read_csv('data/KDDTrain+.txt', names=columns)

# Show first 5 rows
print("\nüìã First 5 rows of data:")
print(df.head())

# ======================
# STEP 2: EXPLORE DATA
# ======================

print("\nüîç STEP 2: Exploring the data...")

# What attacks do we have?
print("\nüìä Attack types in dataset:")
attack_counts = df['attack_type'].value_counts()
print(attack_counts.head(10))

# Create a simple binary label: Normal (0) vs Attack (1)
df['label'] = df['attack_type'].apply(lambda x: 0 if x == 'normal' else 1)

print(f"\nüéØ Binary labels created:")
print(f"   - Normal traffic: {sum(df['label'] == 0)} samples")
print(f"   - Attack traffic: {sum(df['label'] == 1)} samples")

# ======================
# STEP 3: VISUALIZE
# ======================

print("\nüé® STEP 3: Creating visualizations...")

# Create a folder for plots
import os
if not os.path.exists('plots'):
    os.makedirs('plots')

# 1. Pie chart of normal vs attack
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
labels = ['Normal', 'Attack']
sizes = [sum(df['label'] == 0), sum(df['label'] == 1)]
colors = ['green', 'red']
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
plt.title('Normal vs Attack Traffic')

# 2. Bar chart of top 5 attacks
plt.subplot(1, 2, 2)
top_attacks = df['attack_type'].value_counts().head(5)
top_attacks.plot(kind='bar', color='orange')
plt.title('Top 5 Attack Types')
plt.xticks(rotation=45)

plt.tight_layout()
plt.savefig('plots/data_distribution.png', dpi=100)
plt.show()

print("‚úÖ Plots saved to 'plots/data_distribution.png'")

# ======================
# STEP 4: PREPARE DATA
# ======================

print("\n‚öôÔ∏è STEP 4: Preparing data for machine learning...")

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# Handle categorical columns
categorical_cols = ['protocol_type', 'service', 'flag']
label_encoders = {}

for col in categorical_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le
        print(f"   - Encoded {col}")

# Select features (remove text columns)
features = [col for col in df.columns if col not in 
           ['attack_type', 'difficulty_level', 'label']]

X = df[features]
y = df['label']

print(f"\nüìê Features selected: {len(features)}")
print(f"üìè Target variable: 'label' (0=normal, 1=attack)")

# Split data: 70% train, 30% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\n‚úÇÔ∏è Data split:")
print(f"   - Training samples: {X_train.shape[0]}")
print(f"   - Testing samples: {X_test.shape[0]}")

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Data scaled and ready!")

# ======================
# STEP 5: TRAIN MODEL
# ======================

# ============================================
# STEP 5: TRAIN MODEL (Mk II - HYBRID ENSEMBLE)
# ============================================

print("\nü§ñ STEP 5: Training Advanced Hybrid Model...")

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define our two "Expert" models
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced',
    max_depth=15
)

gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)

# Create the "Committee" (Voting Ensemble)
# 'soft' voting means it averages the probability percentages, which is better for our dashboard slider
print("   ...Initializing Voting Ensemble (Random Forest + Gradient Boosting)...")
ensemble_model = VotingClassifier(
    estimators=[
        ('rf', rf_model),
        ('gb', gb_model)
    ],
    voting='soft'
)

# Train the ensemble
print("   ...Fitting model to training data (this may take a moment)...")
ensemble_model.fit(X_train_scaled, y_train)

# Evaluate
print("   ...Evaluating performance...")
y_pred = ensemble_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nüèÜ Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print("   The Hybrid Ensemble is now ready!")

best_model = ensemble_model # Set this as the model to save

# ======================
# STEP 6: EVALUATE
# ======================

print("\nüìà STEP 6: Evaluating the best model...")

# Get predictions from best model
y_pred_best = best_model.predict(X_test_scaled)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_best)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Normal', 'Attack'],
            yticklabels=['Normal', 'Attack'])
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

# Feature Importance (for tree-based models)
if hasattr(best_model, 'feature_importances_'):
    plt.subplot(1, 2, 2)
    importances = best_model.feature_importances_
    top_indices = np.argsort(importances)[-10:]  # Top 10 features
    
    plt.barh(range(10), importances[top_indices])
    plt.yticks(range(10), [features[i] for i in top_indices])
    plt.title('Top 10 Important Features')
    plt.xlabel('Importance Score')

plt.tight_layout()
plt.savefig('plots/model_evaluation.png', dpi=100)
plt.show()

# Classification Report
print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred_best, 
                          target_names=['Normal', 'Attack']))

# ======================
# STEP 7: TEST WITH NEW DATA
# ======================

print("\nüß™ STEP 7: Testing with sample data...")

# Create a fake "new connection" to test
sample_connection = {
    'duration': 0,
    'protocol_type': 'tcp',  # Will be encoded
    'service': 'http',
    'flag': 'SF',
    'src_bytes': 100,
    'dst_bytes': 200,
    'land': 0,
    'wrong_fragment': 0,
    'urgent': 0,
    'hot': 0,
    'num_failed_logins': 0,
    'logged_in': 1,
    'num_compromised': 0,
    'root_shell': 0,
    'su_attempted': 0,
    'num_root': 0,
    'num_file_creations': 0,
    'num_shells': 0,
    'num_access_files': 0,
    'num_outbound_cmds': 0,
    'is_host_login': 0,
    'is_guest_login': 0,
    'count': 1,
    'srv_count': 1,
    'serror_rate': 0.0,
    'srv_serror_rate': 0.0,
    'rerror_rate': 0.0,
    'srv_rerror_rate': 0.0,
    'same_srv_rate': 1.0,
    'diff_srv_rate': 0.0,
    'srv_diff_host_rate': 0.0,
    'dst_host_count': 1,
    'dst_host_srv_count': 1,
    'dst_host_same_srv_rate': 1.0,
    'dst_host_diff_srv_rate': 0.0,
    'dst_host_same_src_port_rate': 0.0,
    'dst_host_srv_diff_host_rate': 0.0,
    'dst_host_serror_rate': 0.0,
    'dst_host_srv_serror_rate': 0.0,
    'dst_host_rerror_rate': 0.0,
    'dst_host_srv_rerror_rate': 0.0
}

# Convert to DataFrame
sample_df = pd.DataFrame([sample_connection])

# Encode categorical variables
for col in categorical_cols:
    if col in sample_df.columns:
        # Use the encoder we already trained
        if col in label_encoders:
            try:
                sample_df[col] = label_encoders[col].transform(sample_df[col])
            except:
                # If new category, assign -1
                sample_df[col] = -1

# Select same features and scale
sample_features = sample_df[features]
sample_scaled = scaler.transform(sample_features)

# Predict
prediction = best_model.predict(sample_scaled)
probability = best_model.predict_proba(sample_scaled)[0]

print(f"\nüîÆ Prediction for sample connection:")
print(f"   - Probability of being NORMAL: {probability[0]:.4f}")
print(f"   - Probability of being ATTACK: {probability[1]:.4f}")
print(f"   - Final prediction: {'üö® ATTACK' if prediction[0] == 1 else '‚úÖ NORMAL'}")

# ======================
# STEP 8: SAVE MODEL
# ======================

print("\nüíæ STEP 8: Saving the model...")

import pickle
import joblib

# Create models folder
if not os.path.exists('models'):
    os.makedirs('models')

# Save the model
model_filename = 'models/best_model.pkl'
joblib.dump(best_model, model_filename)

# Save the scaler
scaler_filename = 'models/scaler.pkl'
joblib.dump(scaler, scaler_filename)

# Save encoders
encoders_filename = 'models/encoders.pkl'
joblib.dump(label_encoders, encoders_filename)

print(f"‚úÖ Model saved to: {model_filename}")
print(f"‚úÖ Scaler saved to: {scaler_filename}")
print(f"‚úÖ Encoders saved to: {encoders_filename}")

# ======================
# FINAL STEP: SUMMARY
# ======================

print("\n" + "="*50)
print("üéâ PROJECT COMPLETED SUCCESSFULLY!")
print("="*50)
print("\nüìä WHAT YOU'VE ACCOMPLISHED:")
print("1. ‚úÖ Downloaded cybersecurity dataset")
print("2. ‚úÖ Explored network traffic data")
print("3. ‚úÖ Created visualizations")
print("4. ‚úÖ Prepared data for ML")
print("5. ‚úÖ Trained 3 machine learning models")
print("6. ‚úÖ Evaluated models (accuracy up to 99%+)")
print("7. ‚úÖ Tested with sample data")
print("8. ‚úÖ Saved trained model for future use")

print("\nüìÅ FILES CREATED:")
print("   - data/KDDTrain+.txt (dataset)")
print("   - plots/data_distribution.png")
print("   - plots/model_evaluation.png")
print("   - models/best_model.pkl (your AI model!)")
print("   - models/scaler.pkl")
print("   - models/encoders.pkl")

print("\nüöÄ NEXT STEPS:")
print("1. Run: python simple_detector.py (again to see it work)")
print("2. Open plots/ folder to see your graphs")
print("3. Show friends: 'I built an AI cybersecurity system!'")

print("\nüí° TIPS:")
print("- Try changing the sample_connection values")
"- Add more features from the dataset"
"- Try different machine learning models"

print("\n" + "="*50)
print("üë®‚Äçüíª You're now a Cybersecurity AI Engineer!")
print("="*50)